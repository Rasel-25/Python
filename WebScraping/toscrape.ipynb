{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(r'https://quotes.toscrape.com')\n",
    "res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bs4 â€” BeautifulSoup 4\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files\n",
    "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "It commonly saves programmers hours or days of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lxml\n",
    "lxml is a Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping. There are a lot of off-the-shelf XML parsers out there, but for better results, developers sometimes prefer to write their own XML and HTML parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(res.text,'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title # getting title with tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text # getting only title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title') # return single tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title').string # text / string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('head') # return multiple tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.link # return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.link.attrs #return link as list \n",
    "soup.link.attrs['href'] #return only link text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', attrs={'class':'quote'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the quote\n",
    "soup.find('div',attrs={'class':'quote'}).span.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the author\n",
    "soup.find('div',attrs={'class':'quote'}).find('small',attrs={'class':'author'}).string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the tags\n",
    "tags=soup.find('div',attrs={'class':'quote'}).find('div',attrs={'class':'tags'}).find_all('a')\n",
    "tags\n",
    "for tag in tags:\n",
    "    print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Python for loop one line with list comprehension\n",
    "   \n",
    "   Syntax \n",
    "   new_list = [expression for member in iterable]\n",
    " \n",
    "    my_list = [n*n for n in range(1, 10)]\n",
    "    print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable string \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    \n",
    "    database.append(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable author \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    author=quote.find('small',attrs={'class':'author'}).string\n",
    "    database.append(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable tags\n",
    "#  \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    #author=quote.find('small',attrs={'class':'author'}).string\n",
    "    tags=[tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a') ]\n",
    "    database.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable  \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    author=quote.find('small',attrs={'class':'author'}).string\n",
    "    tags=[tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a') ]\n",
    "    database.append([q,author,tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('li',attrs={'class':'next'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('li',class_='next').a.attrs['href'] # class_='next' smilat to attrs={'class':'next'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPage(soup:BeautifulSoup):\n",
    "    tempData=[]\n",
    "    for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "        q=quote.span.string\n",
    "        author=quote.find('small',attrs={'class':'author'}).string\n",
    "        tags=[tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a') ]\n",
    "        database.append([q,author,tags])\n",
    "    #return tempData\n",
    "\n",
    "    print(tempData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Tag\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\DATA SCIENCE\\PYTHON\\Python\\WebScraping\\toscrape.ipynb Cell 35'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/DATA%20SCIENCE/PYTHON/Python/WebScraping/toscrape.ipynb#ch0000034?line=8'>9</a>\u001b[0m Database \u001b[39m=\u001b[39m Database \u001b[39m+\u001b[39m extractedQuote\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DATA%20SCIENCE/PYTHON/Python/WebScraping/toscrape.ipynb#ch0000034?line=9'>10</a>\u001b[0m neext\u001b[39m=\u001b[39msoup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mli\u001b[39m\u001b[39m'\u001b[39m,attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m'\u001b[39m}) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/DATA%20SCIENCE/PYTHON/Python/WebScraping/toscrape.ipynb#ch0000034?line=10'>11</a>\u001b[0m url\u001b[39m=\u001b[39mstart_url\u001b[39m+\u001b[39;49mneext\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DATA%20SCIENCE/PYTHON/Python/WebScraping/toscrape.ipynb#ch0000034?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DATA%20SCIENCE/PYTHON/Python/WebScraping/toscrape.ipynb#ch0000034?line=12'>13</a>\u001b[0m resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"Tag\") to str"
     ]
    }
   ],
   "source": [
    "start_url=r'https://quotes.toscrape.com'\n",
    "resp=requests.get(start_url)\n",
    "#resp.content\n",
    "Database = []\n",
    "while True:\n",
    "    try:\n",
    "        soup=BeautifulSoup(resp.content)\n",
    "        extractedQuote=extractPage(soup)\n",
    "        Database = Database + extractedQuote\n",
    "        neext=soup.find('li',attrs={'class':'next'}) \n",
    "        url=start_url+neext\n",
    "        print(url)\n",
    "        resp = requests.get(url)\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print('Successfully all pages Extracted!')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPage(soup: BeautifulSoup):\n",
    "    tempData = []\n",
    "    for quote in soup.findAll('div', attrs={'class': 'quote'}):\n",
    "        q = quote.span.string\n",
    "        author = quote.find('small', attrs={'class':'author'}).string\n",
    "        tags = [tag.string for tag in quote.find('div', attrs={'class':'tags'}).find_all('a')]\n",
    "        tempData.append([q, author, tags])\n",
    "    return tempData\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = r'https://quotes.toscrape.com'\n",
    "resp = requests.get(start_url)\n",
    "Database = []\n",
    "while True:\n",
    "    try:\n",
    "        soup = BeautifulSoup(resp.content)\n",
    "        extractedQuote = extractPage(soup)\n",
    "        Database = Database + extractedQuote\n",
    "        neext = soup.find('li', class_='next').a.attrs['href']\n",
    "        url = start_url + neext\n",
    "        print(url)\n",
    "        resp = requests.get(url)\n",
    "    except AttributeError as e:\n",
    "        print('Successfully all pages Extracted!')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
